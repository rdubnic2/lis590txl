{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nytimesarticle import articleAPI\n",
    "from newspaper import article\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import html2text\n",
    "import requests\n",
    "import os\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import simplejson as json\n",
    "import time\n",
    "from itertools import chain\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nyt_key = os.environ['03ad80825a5a4292ac5d72538642ad21']\n",
    "# nyt_api = articleAPI(nyt_key) \n",
    "\n",
    "# clean_wapo_movie_rev_list = []\n",
    "# clean_bg_movie_rev_list = []\n",
    "# clean_d_rev_list = []\n",
    "clean_d_game_rev_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# url = ('www.destructoid.com/products-index.phtml?filt=reviews&date_s=desc&category=')\n",
    "# r  = requests.get(\"https://\" + url)\n",
    "# data = r.text\n",
    "# soup = BeautifulSoup(data)\n",
    "\n",
    "# d_rev_list = []\n",
    "\n",
    "\n",
    "# for link in soup.find_all('a'):\n",
    "#     url = link.get('href')\n",
    "#     # print(url)\n",
    "#     if url not in d_rev_list:\n",
    "#         d_rev_list.append(url)\n",
    "        \n",
    "# print(len(d_rev_list))\n",
    "\n",
    "# for item in clean_d_game_rev_list:\n",
    "#     print(item)\n",
    "\n",
    "# print(len(clean_bg_movie_rev_list))\n",
    "\n",
    "# bg_movie_rev_list = bg_movie_rev_list[24:105]\n",
    "\n",
    "# for item in clean_d_rev_list:\n",
    "#     str_item = str(item)\n",
    "#     full_url = 'https://www.destructoid.com/' + str_item\n",
    "#     clean_d_game_rev_list.append(full_url)\n",
    "\n",
    "# print(clean_d_game_rev_list)\n",
    "    \n",
    "# for item in d_rev_list:\n",
    "#     str_item = str(item)\n",
    "#     if str_item[0:6] == 'review':\n",
    "#         clean_d_rev_list.append(item)\n",
    "        \n",
    "# print(len(clean_d_rev_list))\n",
    "\n",
    "# for item in wapo_movie_rev_list:\n",
    "#     if item[0:55] == 'https://www.washingtonpost.com/goingoutguide/movies/':\n",
    "#         clean_wapo_movie_rev_list.append(item)\n",
    "    \n",
    "# for item in wapo_movie_rev_list:\n",
    "#     print(item)\n",
    "\n",
    "# clean_wapo_movie_rev_list\n",
    "\n",
    "# print(len(clean_wapo_movie_rev_list))\n",
    "# copy_wapo_movie_list = wapo_movie_rev_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(len(var_movie_rev_list))\n",
    "# for item in var_movie_rev_list:\n",
    "#     str_item = str(item)\n",
    "#     if 'review' not in str_item:\n",
    "#         copy_rs_movie_list.remove(item)\n",
    "\n",
    "# # print(len(copy_rs_movie_list))\n",
    "\n",
    "# for item in rs_movie_rev_list:\n",
    "#     if item[0:4] != 'http':\n",
    "#         copy_rs_movie_list.remove(item)\n",
    "\n",
    "# print(len(copy_rs_movie_list))\n",
    "\n",
    "# copy_rs_movie_list\n",
    "\n",
    "# clean_copy_lat_movie_list = []\n",
    "\n",
    "# for item in copy_lat_movie_list:\n",
    "#     str_item = str(item)\n",
    "#     full_url = 'http://www.latimes.com/' + str_item\n",
    "#     clean_copy_lat_movie_list.append(full_url)\n",
    "    \n",
    "# for item in clean_copy_lat_movie_list[3:45]:\n",
    "#     print(item)\n",
    "\n",
    "# print(len(clean_copy_lat_movie_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# import requests\n",
    "\n",
    "# url = input('Enter a website to extract the URLs from: ')\n",
    "# r  = requests.get(\"http://\" +url)\n",
    "# data = r.text\n",
    "# soup = BeautifulSoup(data)\n",
    "\n",
    "# for link in soup.find_all('a'):\n",
    "#     print(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print('Pre-cleaning count:')\n",
    "# print(len(nyt_movie_rev_list))\n",
    "# # nyt_book_rev_list\n",
    "\n",
    "# for item in nyt_movie_rev_list:\n",
    "#     if item[0:4] != '/201':\n",
    "#         nyt_movie_rev_list.remove(item)\n",
    "        \n",
    "# print('Post-cleaning count:')\n",
    "# print(len(nyt_movie_rev_list))\n",
    "\n",
    "# clean_nyt_movie_rev_list = nyt_movie_rev_list[0:21]\n",
    "# clean_nyt_movie_rev_list\n",
    "\n",
    "# clean_lat_movie_rev_list2 = []\n",
    "\n",
    "# for item in clean_nyt_movie_rev_list:\n",
    "#     str_item = str(item)\n",
    "#     full_url = 'https://www.nytimes.com/' + str_item\n",
    "#     clean_nyt_movie_rev_list2.append(full_url)\n",
    "\n",
    "# for item in clean_nyt_movie_rev_list2:\n",
    "#     print(item)\n",
    "    \n",
    "\n",
    "# clean_nyt_movie_rev_list2\n",
    "# for item2 in nyt_book_rev_list2:\n",
    "#     if item2[0:26] == 'https://www.nytimes.com/20':\n",
    "#         clean_nyt_book_rev_list2.append(item2)\n",
    "\n",
    "# print(len(clean_nyt_book_rev_list2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply import and initialize the API with your developer key::\n",
    "\n",
    "from nytimesarticle import articleAPI\n",
    "api = articleAPI('*Your Key*')\n",
    "\n",
    "And call the 'search' function with your desired search parameters/values::\n",
    "\n",
    "articles = api.search( q = 'Obama', fq = {'headline':'Obama', 'source':['Reuters','AP', 'The New York Times']}, \n",
    "                      begin_date = 20111231, facet_field = ['source','day_of_week'], facet_filter = True )\n",
    "\n",
    "The search function returns a dictionary of the search results.\n",
    "\n",
    "You can specify multiple filters by using a dictionary::\n",
    "fq = {'headline':'Obama', 'source':['Reuters','AP', 'The New York Times']}\n",
    "\n",
    "And multiple values by using a list::\n",
    "facet_field = ['source','day_of_week']\n",
    "\n",
    "More examples::\n",
    "\n",
    "articles = api.search( q = 'Obama' )\n",
    "articles = api.search( q = 'Obama', begin_date = 20111231, page=2 )\n",
    "\n",
    "For a complete overview of the available search parameters, please refer to the NYTimes Article Search API \n",
    "`Documentation <http: developer.nytimes.com=\"\" docs=\"\" read=\"\" article_search_api_v2=\"\">`_.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movie_api = articleAPI('03ad80825a5a4292ac5d72538642ad21')\n",
    "\n",
    "# articles = movie_api.search(q = 'Obama', fq = {'headline':'Obama', 'source':['Reuters','AP', 'The New York Times']}, begin_date = 20111231, facet_field = ['source','day_of_week'], facet_filter = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nyt_book_review = newspaper.build('https://www.nytimes.com/section/books/review')\n",
    "# ny_review_books = newspaper.build('https://www.nyrb.com/collections/new-york-review-books')\n",
    "# nyt_movie_review = newspaper.build('https://www.nytimes.com/reviews/movies', memoize_articles=True)\n",
    "# cnn_test = newspaper.build('https://www.cnn.com', memoize_articles=True)\n",
    "# sp_test = newspaper.build('http://smilepolitely.com/music/', memoize_articles=True)\n",
    "# ng_test = newspaper.build('http://www.news-gazette.com/living', memoize_articles=True)\n",
    "# stl_test = newspaper.build('http://www.stltoday.com/sports/', memoize_articles=True)\n",
    "\n",
    "# print(len(stl_test.articles))\n",
    "# print(type(nyt_book_review))\n",
    "# print(type(nyt_movie_review))\n",
    "\n",
    "# nyt_book_review_list = []\n",
    "# nyt_book_review_text = []\n",
    "# nyt_movie_review_list = []\n",
    "# ny_review_books_list = []\n",
    "# nyt_movie_review_text = []\n",
    "\n",
    "# cnn_article_links = []\n",
    "# cnn_article_text = []\n",
    "# cnn_article_links2 = []\n",
    "# cnn_article_text2 = []\n",
    "\n",
    "# sp_test_links = []\n",
    "# sp_test_text = []\n",
    "\n",
    "# ng_test_links = []\n",
    "# ng_test_text = []\n",
    "\n",
    "# stl_test_links = []\n",
    "# stl_test_text = []\n",
    "\n",
    "# stl_test_links2 = []\n",
    "# stl_test_text2 = []\n",
    "\n",
    "# for art in stl_test.articles:\n",
    "#     time.sleep(0.5)\n",
    "# #     article_parsed = a.parse()\n",
    "# #     print(article_url)\n",
    "#     art.download()\n",
    "#     art.parse()\n",
    "#     text = art.text\n",
    "#     # stl_test_links2.append(art_url)\n",
    "#     stl_test_text2.append(text)\n",
    "\n",
    "# for article in sp_test.articles:\n",
    "#     article.parse()\n",
    "#     print(article.url)\n",
    "#     print(article.text)\n",
    "#     # print(type(article.url))\n",
    "#     sp_test_links.append(article.url)\n",
    "#     sp_test_text.append(article.text)\n",
    "    \n",
    "# print(len(cnn_article_links2))\n",
    "\n",
    "# for article in ny_review_books.articles:\n",
    "#     # print(article.url)\n",
    "#     # print(type(article.url))\n",
    "#     ny_review_books_list.append(article.url)\n",
    "#     # nyt_movie_review_text.append(article.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first_article = stl_test.articles[85]\n",
    "# first_article.download()\n",
    "# first_article.parse()\n",
    "# first_article.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for link in stl_test_links:\n",
    "#     x = link.download()\n",
    "#     y = x.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 movie reviews from NYT\n"
     ]
    }
   ],
   "source": [
    "# len(nyt_book_review_list)\n",
    "# print('Found', len(nyt_movie_review_list), 'movie reviews from NYT')\n",
    "# print('Scraped text from', len(nyt_movie_review_text), 'movie reviews from NYT')\n",
    "# print('Found', len(nyt_book_review_list), 'book reviews from NYT')\n",
    "# print('Scraped text from', len(nyt_book_review_text), 'book reviews from NYT')\n",
    "\n",
    "# print('Found', len(cnn_article_links), 'articles from CNN')\n",
    "# print('Scraped text from', len(cnn_article_text), 'articles from CNN')\n",
    "\n",
    "# print('Found', len(sp_test_links), 'articles from Smile Politely')\n",
    "# print('Scraped text from', len(sp_test_text), 'articles from Smile Politely')\n",
    "\n",
    "# print('Found', len(ng_test_links), 'articles from N-G')\n",
    "# print('Scraped text from', len(ng_test_text), 'articles from N-G')\n",
    "\n",
    "# print('Found', len(stl_test_links), 'articles from STL')\n",
    "# print('Scraped text from', len(stl_test_text2), 'articles from STL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from bs4.diagnose import diagnose\n",
    "import requests\n",
    "\n",
    "def get_text(url):\n",
    "    data = ''\n",
    "    p = requests.get(url).content\n",
    "    # print('p = ', p)\n",
    "    soup = BeautifulSoup(p, 'lxml')\n",
    "    # print('soup =', soup)\n",
    "    paragraphs = soup.select('p')\n",
    "    # print('\\n paragraphs: \\n', paragraphs)\n",
    "    data = p\n",
    "    # print('\\n data = \\n',data)\n",
    "    return(paragraphs)\n",
    "#     text = \"\"\n",
    "#     for paragraph in paragraphs:\n",
    "#         text = ' '.join([paragraph.rstrip('\\n') for paragraph in paragraphs])\n",
    "#         # text += paragraph.text\n",
    "#         # print('text =', text)\n",
    "#         return str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with open(\"partial_video_games.txt\", \"r\") as text:\n",
    "#     for line in text:\n",
    "#         time.sleep(10)\n",
    "#         review = get_text(line)\n",
    "#         partial_vg.append(review)\n",
    "\n",
    "# print(partial_vg)\n",
    "\n",
    "# get_text('http://www.gameinformer.com/themes/blogs/generic/post.aspx?WeblogApp=playstation4&y=2017&m=04&d=19&WeblogPostName=one-classy-paint-job&GroupKeys=games/full_throttle_remastered/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with open(\"videogame_reviews.txt\", \"r\") as text:\n",
    "#     for line in text:\n",
    "#         print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# <p class=\"story-body-text story-content\"\n",
    "\n",
    "# get_text('https://www.nytimes.com/2017/04/19/movies/tomorrow-review.html?rref=collection%2Fcollection%2Fmovie-guide&action=click&contentCollection=undefined&region=stream&module=stream_unit&version=latest-stories&contentPlacement=1&pgtype=collection')\n",
    "# movie_rev = []\n",
    "\n",
    "# in_text  = open('movie_review_urls.txt', 'r')\n",
    "\n",
    "# in_text_lines = in_text.readlines()\n",
    "\n",
    "# for line in in_text_lines:\n",
    "#     url = str(line)\n",
    "#     # time.sleep(1)\n",
    "#     try:\n",
    "#         url_text = get_text(url)\n",
    "#         # print(url_text)\n",
    "#         movie_rev.append(url_text)\n",
    "#     except:\n",
    "#         print('Something went wrong with', url_text)\n",
    "\n",
    "# print(len(movie_rev))\n",
    "\n",
    "# with open(\"partial_video_game_reviews.txt\", \"r\") as text:\n",
    "    # text_lines = text.readlines()\n",
    "    # print(text)\n",
    "\n",
    "# with open('movie_rev_text.txt', 'w') as output:\n",
    "#     for line in movie_rev:\n",
    "#         str_line = str(line)\n",
    "#         output.write(str_line)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(len(ny_review_books_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?re.sub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "clean_ny_review_books_text = []\n",
    "for article in ny_review_books_text:\n",
    "    clean_article = re.sub('<[^<]+?>', '', str(article))\n",
    "    clean_ny_review_books_text.append(clean_article)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-4ad37bfffc56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_nyrb_list_copy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnyrb_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_nyrb_list_copy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# for item in clean_nyrb_list_copy:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "clean_nyrb_list_copy = clean_ny_review_books_text\n",
    "len(clean_nyrb_list_copy)\n",
    "\n",
    "for item in clean_nyrb_list_copy:\n",
    "    if len(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nyrb_file = open('nyrb_reviews.txt', 'w')\n",
    "\n",
    "for item in clean_ny_review_books_text:\n",
    "    nyrb_file.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MY REGEX TO REMOVE ANGLE BRACKET CHUNKS\n",
    "\n",
    "# \\<.*?\\> then , '') \n",
    "\n",
    "clean_mov  = open('clean_movie_rev_text.txt', 'w')\n",
    "\n",
    "with open('movie_rev_text.txt', encoding ='utf-8') as f:\n",
    "    for line in f:\n",
    "            clean = re.sub('<[^<]+?>', '', str(line))\n",
    "            clean_mov.write(clean)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
