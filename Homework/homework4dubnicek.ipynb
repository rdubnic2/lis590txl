{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully by this week, you're comfortable with lists (i.e. [a, b, c, ..]) and growing to understand list comprehensions.\n",
    "\n",
    "Two fundamental Python skills to be aware of. First, there's a general purpose method called `len()` that returns the length of an object, like \"how many items in this list\" or \"how many characters in this string\". e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = ['hello', 'text', 'mining']\n",
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"Text Mining\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also an object called a `set`, which is like a list, but without an ordering and only allowing unique elements. This is useful for us, because it gives a quick way to see just the unique words of a list: the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List: ['Buffalo', 'buffalo', 'Buffalo', 'buffalo', 'buffalo', 'Buffalo', 'buffalo', 'buffalo']\n",
      "Set: {'buffalo', 'Buffalo'}\n"
     ]
    }
   ],
   "source": [
    "l = ['Buffalo', 'buffalo', 'Buffalo', 'buffalo', 'buffalo', 'Buffalo', 'buffalo', 'buffalo']\n",
    "s = set(l)\n",
    "print(\"List:\", l)\n",
    "print(\"Set:\", s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week, follow along with [Searching for Meaning](https://github.com/sgsinclair/alta/blob/41f389f3d9708573c44c883bcd95fd16bad54a24/ipynb/SearchingMeaning.ipynb) from the Art of Literary Text Analysis.\n",
    "\n",
    "Use the trimmed version of Frankenstein from last week to try some of the concepts in the chapter. This should get you up to speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85440\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "with open('../data/frankenstein.txt') as f:\n",
    "    frankensteinString = f.read()\n",
    "frankensteinTokens = nltk.word_tokenize(frankensteinString)\n",
    "cleanedTokens = [word.lower() for word in frankensteinTokens if word[0].isalpha()]\n",
    "print(len(frankensteinTokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that there are 85440 tokens in the text. If we count just the _unique_ words (the _vocabulary size_), we find 7510:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7038"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(cleanedTokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1) Create a lemmatized version of cleanedTokens and count the unique lemmas. Share the code to do this: the answer that it gives you should be 6417."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6416"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "frankensteinLemmas = [wnl.lemmatize(word) for word in cleanedTokens]\n",
    "len(set(frankensteinLemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2) Re-do the lemmatization after stopping words against the default NLTK stoplist, and tabulate the top ten words. Paste the code and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    one   could   would     yet     man  father     day    upon  friend    life     eye thought    time   every   might \n",
      "    207     197     183     152     136     131     129     126     125     124     122     112     109     109     108 \n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "cleanedTokensNoStop = [word for word in cleanedTokens if word not in stopwords]\n",
    "\n",
    "frankensteinLemmasNoStop = [wnl.lemmatize(word) for word in cleanedTokensNoStop]\n",
    "frankenLemmasNoStopFreq = nltk.FreqDist(frankensteinLemmasNoStop)\n",
    "frankenLemmasNoStopFreq.tabulate(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3) How does the tabulation of lemmas differ from the tabulation of the non-lemmatized (but still stopped and case-folded) tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   one  could  would    yet    man father   upon   life  every  might  first  shall   eyes   said    may \n",
      "   206    197    183    152    136    131    126    115    109    108    108    105    104    102     98 \n"
     ]
    }
   ],
   "source": [
    "cleanedTokensNoStopFreq = nltk.FreqDist(cleanedTokensNoStop)\n",
    "cleanedTokensNoStopFreq.tabulate(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 4) What are the WordNet synsets for 'monster'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('monster.n.01'),\n",
       " Synset('giant.n.05'),\n",
       " Synset('freak.n.01'),\n",
       " Synset('monster.n.04'),\n",
       " Synset('monster.n.05')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "wn.synsets('monster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 5) A synset has a method called `definition()`. Noting that the code for Q4 resulted in a list, write a list comprehension to extracts all the definitions for each synset. Share the code and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['an imaginary creature usually having various human and animal parts', 'someone or something that is abnormally large and powerful', 'a person or animal that is markedly unusual or deformed', 'a cruel wicked and inhuman person', '(medicine) a grossly malformed and usually nonviable fetus']\n"
     ]
    }
   ],
   "source": [
    "# print(len(cleanedTokensNoStop))\n",
    "frankensteinSynsets = [wn.synsets(token) for token in cleanedTokensNoStop]\n",
    "# print(len(frankensteinSynsets))\n",
    "\n",
    "monsterDefs = [synset.definition() for synset in wn.synsets('monster')]\n",
    "print(monsterDefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 6) Each synset is a child of a more general synset. For example, `crab` is an example of a `decapod_crustacean`, which is more generally a `crustacean`, and so on. You can get at the paths to the root of this tree with `hypernym_paths()`. Paste the code and hypernym path for `freak.n.01`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Synset('entity.n.01'),\n",
       "  Synset('physical_entity.n.01'),\n",
       "  Synset('object.n.01'),\n",
       "  Synset('whole.n.02'),\n",
       "  Synset('living_thing.n.01'),\n",
       "  Synset('organism.n.01'),\n",
       "  Synset('mutant.n.01'),\n",
       "  Synset('freak.n.01')]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freakSynset = wn.synset('freak.n.01')\n",
    "freakSynset.hypernym_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 7) (for 2 points): We've already seen some corpora that NLTK can pull in, from the complex WordNet information to a basic stoplist. Using the NLTK information on male/female names, determine and paste in the unique female names in Frankenstein. This isn't in the ALTA book, but searching Google sometimes helps ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  7944  total names in this corpus\n",
      "There are  2943  male names in this corpus\n",
      "There are  5001  female names in this corpus\n",
      "{'France', 'Angelica', 'Asia', 'Geneva', 'Justine', 'Lacey', 'Christian', 'Louisa', 'Angel', 'La', 'Abbey', 'Manon', 'Daniel', 'Isis', 'Agatha', 'Margaret', 'Happy', 'Harmony', 'Caroline', 'May', 'June', 'Eve', 'Elizabeth'}\n"
     ]
    }
   ],
   "source": [
    "cleanedTokens2 = [word for word in frankensteinTokens if word[0].isalpha()]\n",
    "names = nltk.corpus.names\n",
    "male_names = names.words('male.txt')\n",
    "female_names = names.words('female.txt')\n",
    "namelist = male_names + female_names\n",
    "print('There are ', len(namelist), ' total names in this corpus')\n",
    "print('There are ', len(male_names), ' male names in this corpus')\n",
    "print('There are ', len(female_names), ' female names in this corpus')\n",
    "\n",
    "# frankenSteinNames = [word for word in cleanedTokens2 if word in namelist]\n",
    "# print('There are ', len(frankenSteinNames), ' many names in Frankenstein')\n",
    "\n",
    "frankenSteinFemaleNames = [word for word in cleanedTokens2 if word in female_names]\n",
    "print(set(frankenSteinFemaleNames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our copy of Frankenstein is from Project Gutenberg, a collection of transcriptions of public domain (i.e. legally shareable) books. NLTK offers a small selection of those books through `nltk.corpus.gutenberg`.\n",
    "\n",
    "Load the gutenberg corpus and convert it to what NLTK calls a TextCollection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF for 'monster':  8.60294946704625e-05\n",
      "TF-IDF for 'miserable':  6.06154057055602e-05\n",
      "TF-IDF for 'horror':  3.938280089547152e-05\n"
     ]
    }
   ],
   "source": [
    "from nltk.text import TextCollection\n",
    "gutenberg_docs = nltk.corpus.gutenberg\n",
    "gutenberg_collection = TextCollection(gutenberg_docs)\n",
    "\n",
    "print(\"TF-IDF for 'monster': \", gutenberg_collection.tf_idf('monster', frankensteinString))\n",
    "print(\"TF-IDF for 'miserable': \", gutenberg_collection.tf_idf('miserable', frankensteinString))\n",
    "print(\"TF-IDF for 'horror': \", gutenberg_collection.tf_idf('horror', frankensteinString))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a method of `gutenberg_collection` (remember auto-complete!), answer the final two-part question.\n",
    "\n",
    "Questions:\n",
    "    \n",
    " - 8) For 2 points:\n",
    "   - What is the TFIDF for 'monster' in Frankenstein? You'll need the original string.\n",
    "   - What word has the highest TF-IDF for the following: 'miserable', 'horror', 'monster'? If you need it, you can compare numbers in python with > (greater than) or < (less than)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
