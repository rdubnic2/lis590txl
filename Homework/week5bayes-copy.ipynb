{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5: A Naive Bayes classifier\n",
    "\n",
    "Suppose President Trump gets savvy, and realizes that people can use the Android/iPhone distinction to separate his tweets from the the tweets of aides. He starts using an iPhone too. Now, how will we distinguish tweets really authored by the President?\n",
    "\n",
    "Well, one thing we can do is train a classifier to predict authorship using the text itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/tunder/Dropbox/courses/2017datasci/code\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>text</th>\n",
       "      <th>favorited</th>\n",
       "      <th>favoriteCount</th>\n",
       "      <th>replyToSN</th>\n",
       "      <th>created</th>\n",
       "      <th>truncated</th>\n",
       "      <th>replyToSID</th>\n",
       "      <th>id</th>\n",
       "      <th>replyToUID</th>\n",
       "      <th>statusSource</th>\n",
       "      <th>screenName</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>isRetweet</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>My economic policy speech will be carried live...</td>\n",
       "      <td>False</td>\n",
       "      <td>9214</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-08-08 15:20:44</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>762669882571980801</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>3107</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Join me in Fayetteville, North Carolina tomorr...</td>\n",
       "      <td>False</td>\n",
       "      <td>6981</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-08-08 13:28:20</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>762641595439190016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>2390</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>#ICYMI: \"Will Media Apologize to Trump?\" https...</td>\n",
       "      <td>False</td>\n",
       "      <td>15724</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-08-08 00:05:54</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>762439658911338496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>6691</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Michael Morell, the lightweight former Acting ...</td>\n",
       "      <td>False</td>\n",
       "      <td>19837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-08-07 23:09:08</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>762425371874557952</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>6402</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>The media is going crazy. They totally distort...</td>\n",
       "      <td>False</td>\n",
       "      <td>34051</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-08-07 21:31:46</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>762400869858115588</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>11717</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  \\\n",
       "0           0             1   \n",
       "1           1             2   \n",
       "2           2             3   \n",
       "3           3             4   \n",
       "4           4             5   \n",
       "\n",
       "                                                text favorited  favoriteCount  \\\n",
       "0  My economic policy speech will be carried live...     False           9214   \n",
       "1  Join me in Fayetteville, North Carolina tomorr...     False           6981   \n",
       "2  #ICYMI: \"Will Media Apologize to Trump?\" https...     False          15724   \n",
       "3  Michael Morell, the lightweight former Acting ...     False          19837   \n",
       "4  The media is going crazy. They totally distort...     False          34051   \n",
       "\n",
       "  replyToSN              created truncated  replyToSID                  id  \\\n",
       "0       NaN  2016-08-08 15:20:44     False         NaN  762669882571980801   \n",
       "1       NaN  2016-08-08 13:28:20     False         NaN  762641595439190016   \n",
       "2       NaN  2016-08-08 00:05:54     False         NaN  762439658911338496   \n",
       "3       NaN  2016-08-07 23:09:08     False         NaN  762425371874557952   \n",
       "4       NaN  2016-08-07 21:31:46     False         NaN  762400869858115588   \n",
       "\n",
       "   replyToUID                                       statusSource  \\\n",
       "0         NaN  <a href=\"http://twitter.com/download/android\" ...   \n",
       "1         NaN  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "2         NaN  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "3         NaN  <a href=\"http://twitter.com/download/android\" ...   \n",
       "4         NaN  <a href=\"http://twitter.com/download/android\" ...   \n",
       "\n",
       "        screenName  retweetCount isRetweet retweeted  longitude  latitude  \n",
       "0  realDonaldTrump          3107     False     False        NaN       NaN  \n",
       "1  realDonaldTrump          2390     False     False        NaN       NaN  \n",
       "2  realDonaldTrump          6691     False     False        NaN       NaN  \n",
       "3  realDonaldTrump          6402     False     False        NaN       NaN  \n",
       "4  realDonaldTrump         11717     False     False        NaN       NaN  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, csv, math, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print('Current working directory: ' + cwd + '\\n')\n",
    "      \n",
    "relativepath = os.path.join('..', 'data', 'weekfour', 'trump.csv')\n",
    "trump = pd.read_csv(relativepath)\n",
    "trump.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simplify the Trump dataframe. We only need three columns:\n",
    "\n",
    "1. Text of the tweet\n",
    "2. Source = Android or iphone\n",
    "3. A random number 0-4 that we'll use to divide the dataset into five 'folds'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My economic policy speech will be carried live...</td>\n",
       "      <td>android</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Join me in Fayetteville, North Carolina tomorr...</td>\n",
       "      <td>iphone</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#ICYMI: \"Will Media Apologize to Trump?\" https...</td>\n",
       "      <td>iphone</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Michael Morell, the lightweight former Acting ...</td>\n",
       "      <td>android</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The media is going crazy. They totally distort...</td>\n",
       "      <td>android</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   source  fold\n",
       "0  My economic policy speech will be carried live...  android     1\n",
       "1  Join me in Fayetteville, North Carolina tomorr...   iphone     3\n",
       "2  #ICYMI: \"Will Media Apologize to Trump?\" https...   iphone     4\n",
       "3  Michael Morell, the lightweight former Acting ...  android     2\n",
       "4  The media is going crazy. They totally distort...  android     2"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def trump_test(a_data_frame, rowidx):\n",
    "    if 'iphone' in a_data_frame['statusSource'][rowidx]:\n",
    "        return 'iphone'\n",
    "    elif 'android' in a_data_frame['statusSource'][rowidx]:\n",
    "        return 'android'\n",
    "    else:\n",
    "        return 'other'\n",
    "    \n",
    "tweet_text = trump['text']\n",
    "\n",
    "source = []\n",
    "fold = []\n",
    "for idx in trump.index:\n",
    "    source.append(trump_test(trump, idx))\n",
    "    fold.append(random.sample(list(range(5)), 1)[0])\n",
    "source = pd.Series(source, index = trump.index)\n",
    "fold = pd.Series(fold, index = trump.index)\n",
    "\n",
    "tdf = pd.concat([tweet_text, source, fold], axis = 1)\n",
    "tdf.columns = ['text', 'source', 'fold']\n",
    "\n",
    "# limit the dataframe to columns with either android or iphone;\n",
    "# exclude 'other'\n",
    "tdf = tdf[(tdf['source'] == 'android') | (tdf['source'] == 'iphone')]\n",
    "tdf.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to divide the dataset into a training set and a test set. This is easily done using the \"folds.\" We select one fold as our test set and use all the others as a training set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set includes 1101\n",
      "Test set includes 289\n"
     ]
    }
   ],
   "source": [
    "testset = tdf[tdf['fold'] == 4]\n",
    "trainingset = tdf[tdf['fold'] != 4]\n",
    "print('Training set includes ' + str(trainingset.shape[0]))\n",
    "print('Test set includes ' + str(testset.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need some basic text-wrangling functions that we've used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(astring):\n",
    "    ''' Breaks a string into words, and counts them.\n",
    "    Designed so it strips punctuation and lowercases everything,\n",
    "    but doesn't separate hashtags and at-signs.\n",
    "    '''\n",
    "    wordcounts = Counter()\n",
    "    # create a counter to hold the counts\n",
    "    \n",
    "    tokens = astring.split()\n",
    "    for t in tokens:\n",
    "        word = t.strip(',.!?:;-—()<>[]/\"\\'').lower()\n",
    "        wordcounts[word] += 1\n",
    "        \n",
    "    return wordcounts\n",
    "\n",
    "def create_vocab(seq_of_strings, n):\n",
    "    ''' Given a sequence of text snippets, this function\n",
    "    returns the n most common words. We'll use this to\n",
    "    create a limited 'vocabulary'.\n",
    "    '''\n",
    "    vocab = Counter()\n",
    "    for astring in seq_of_strings:\n",
    "        counts = tokenize(astring)\n",
    "        vocab = vocab + counts\n",
    "    topn = [x[0] for x in vocab.most_common(n)]\n",
    "    return topn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can actually write functions that build a Naive Bayes model. ```train_nb_model``` is the central function here. It calls the other two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source\n",
      "android    606\n",
      "iphone     495\n",
      "Name: text, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neg</th>\n",
       "      <th>pos</th>\n",
       "      <th>all_prob</th>\n",
       "      <th>neg_prob</th>\n",
       "      <th>pos_prob</th>\n",
       "      <th>neg_norm</th>\n",
       "      <th>pos_norm</th>\n",
       "      <th>log_neg</th>\n",
       "      <th>log_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>167</td>\n",
       "      <td>497</td>\n",
       "      <td>0.075558</td>\n",
       "      <td>0.056783</td>\n",
       "      <td>0.085001</td>\n",
       "      <td>0.751525</td>\n",
       "      <td>1.124981</td>\n",
       "      <td>-0.285651</td>\n",
       "      <td>0.117766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>131</td>\n",
       "      <td>252</td>\n",
       "      <td>0.043582</td>\n",
       "      <td>0.044543</td>\n",
       "      <td>0.043099</td>\n",
       "      <td>1.022039</td>\n",
       "      <td>0.988914</td>\n",
       "      <td>0.021800</td>\n",
       "      <td>-0.011147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>77</td>\n",
       "      <td>298</td>\n",
       "      <td>0.042672</td>\n",
       "      <td>0.026182</td>\n",
       "      <td>0.050966</td>\n",
       "      <td>0.613556</td>\n",
       "      <td>1.194378</td>\n",
       "      <td>-0.488483</td>\n",
       "      <td>0.177626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>107</td>\n",
       "      <td>198</td>\n",
       "      <td>0.034706</td>\n",
       "      <td>0.036382</td>\n",
       "      <td>0.033864</td>\n",
       "      <td>1.048284</td>\n",
       "      <td>0.975713</td>\n",
       "      <td>0.047155</td>\n",
       "      <td>-0.024586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>98</td>\n",
       "      <td>207</td>\n",
       "      <td>0.034706</td>\n",
       "      <td>0.033322</td>\n",
       "      <td>0.035403</td>\n",
       "      <td>0.960111</td>\n",
       "      <td>1.020064</td>\n",
       "      <td>-0.040707</td>\n",
       "      <td>0.019865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     neg  pos  all_prob  neg_prob  pos_prob  neg_norm  pos_norm   log_neg  \\\n",
       "the  167  497  0.075558  0.056783  0.085001  0.751525  1.124981 -0.285651   \n",
       "to   131  252  0.043582  0.044543  0.043099  1.022039  0.988914  0.021800   \n",
       "and   77  298  0.042672  0.026182  0.050966  0.613556  1.194378 -0.488483   \n",
       "in   107  198  0.034706  0.036382  0.033864  1.048284  0.975713  0.047155   \n",
       "a     98  207  0.034706  0.033322  0.035403  0.960111  1.020064 -0.040707   \n",
       "\n",
       "      log_pos  \n",
       "the  0.117766  \n",
       "to  -0.011147  \n",
       "and  0.177626  \n",
       "in  -0.024586  \n",
       "a    0.019865  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def categorize(df, rowidx):\n",
    "    if df.loc[rowidx, 'source'] == 'android':\n",
    "        return 'positive'\n",
    "    elif df.loc[rowidx, 'source'] == 'iphone':\n",
    "        return 'negative'\n",
    "    else:\n",
    "        print('error: neither iphone nor android')\n",
    "        return 'other'\n",
    "\n",
    "def get_priors(df):\n",
    "    source_counts = df.groupby('source').count()['text']\n",
    "    print(source_counts)\n",
    "    positive_odds = source_counts['android'] / source_counts['iphone']\n",
    "    negative_odds = source_counts['iphone'] / source_counts['android']\n",
    "    return math.log(positive_odds), math.log(negative_odds)\n",
    "\n",
    "def train_nb_model(df, p):\n",
    "    vocab = create_vocab(df['text'], p)\n",
    "    vocabset = set(vocab)\n",
    "    # we make a set because membership-checking is faster\n",
    "    # in sets; but we also hold onto the list, which is ordered\n",
    "    \n",
    "    positive_prior, negative_prior = get_priors(df)\n",
    "    \n",
    "    positive_counts = Counter()\n",
    "    negative_counts = Counter()\n",
    "    \n",
    "    for i in df.index:\n",
    "        tweet = df['text'][i]\n",
    "        tweet_counts = tokenize(tweet)\n",
    "        category = categorize(df, i)\n",
    "        if category == 'negative':\n",
    "            negative_counts = negative_counts + tweet_counts\n",
    "        elif category == 'positive':\n",
    "            positive_counts = positive_counts + tweet_counts\n",
    "    \n",
    "    # Now let's organize these Counters into a DataFrame\n",
    "    \n",
    "    negative = pd.Series(1, index = vocab)\n",
    "    positive = pd.Series(1, index = vocab)\n",
    "    # notice initializing to 1 -- Laplacian smoothing\n",
    "    \n",
    "    for word, count in positive_counts.items():\n",
    "        if word in vocabset:\n",
    "            positive[word] += count\n",
    "    \n",
    "    for word, count in negative_counts.items():\n",
    "        if word in vocabset:\n",
    "            negative[word] += count\n",
    "    \n",
    "    all_prob = (negative + positive) / (np.sum(negative) + np.sum(positive))\n",
    "    \n",
    "    negative_prob = negative / np.sum(negative)\n",
    "    positive_prob = positive / np.sum(positive)\n",
    "    \n",
    "    # note that when we sum up the negative and positive\n",
    "    # columns, we are also summing up all the Laplacian 1's\n",
    "    # we initially added to them\n",
    "    \n",
    "    model = pd.concat([negative, positive, all_prob, \n",
    "                       negative_prob, positive_prob], axis = 1) \n",
    "        \n",
    "    model.columns = ['neg', 'pos', 'all_prob', 'neg_prob', 'pos_prob']\n",
    "    \n",
    "    # The next step is unnecessary, and will not be found in\n",
    "    # most published versions of naive Bayes. I'm providing it\n",
    "    # because it may help you understand the logic of the\n",
    "    # algorithm.\n",
    "    \n",
    "    model['neg_norm'] = negative_prob / all_prob\n",
    "    model['pos_norm'] = positive_prob / all_prob\n",
    "    \n",
    "    \n",
    "    model['log_neg'] = [math.log(x) for x in model['neg_norm']]\n",
    "    model['log_pos'] = [math.log(x) for x in model['pos_norm']]\n",
    "    return vocab, positive_prior, negative_prior, model\n",
    "\n",
    "vocab, positive_prior, negative_prior, model = train_nb_model(trainingset, 75)\n",
    "model.head() \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we're using logarithms of the probabilities, so that we can just add them up. Our priors are logarithms, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20232222350062407 -0.2023222235006242\n"
     ]
    }
   ],
   "source": [
    "print(positive_prior, negative_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's write a function that applies a given model to a given testset. It will have lots of arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 205 rows right, and 84 wrong.\n",
      "Accuracy was 70.93%\n"
     ]
    }
   ],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "def apply_model(vocab, positive_prior, negative_prior, model, testset):\n",
    "    right = 0\n",
    "    wrong = 0\n",
    "    vocabset = set(vocab)\n",
    "    odds_pos = []\n",
    "    odds_neg = []\n",
    "\n",
    "    for i in testset.index:\n",
    "        odds_positive = positive_prior\n",
    "        odds_negative = negative_prior\n",
    "        tweet = testset['text'][i]\n",
    "        tweet_counts = tokenize(tweet)\n",
    "        for word, count in tweet_counts.items():\n",
    "            if word not in vocabset:\n",
    "                continue\n",
    "            odds_positive += model.loc[word, 'log_pos']\n",
    "            odds_negative += model.loc[word, 'log_neg']\n",
    "            \n",
    "        if odds_positive > odds_negative:\n",
    "            prediction = 'positive'\n",
    "        else:\n",
    "            prediction = 'negative'\n",
    "        \n",
    "        odds_pos.append(odds_positive)\n",
    "        odds_neg.append(odds_negative)\n",
    "\n",
    "        reality = categorize(testset, i)\n",
    "        if reality != 'positive' and reality != 'negative':\n",
    "            continue\n",
    "        elif prediction == reality:\n",
    "            right += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "\n",
    "    print(\"Got \" + str(right) + \" rows right, and \" + str(wrong) + \" wrong.\")\n",
    "    accuracy = (right / (wrong + right)) * 100\n",
    "    print(\"Accuracy was {0:.2f}%\".format(accuracy))\n",
    "    \n",
    "    resultset = testset.copy()\n",
    "    resultset['odds_positive'] = odds_pos\n",
    "    resultset['odds_negative'] = odds_neg\n",
    "    resultset = resultset.sort_values(by = 'odds_positive')\n",
    "    \n",
    "    return resultset\n",
    "\n",
    "newtestset = apply_model(vocab, positive_prior, \n",
    "                         negative_prior, model, testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```apply_model``` function returns a version of the test set with two new columns. The dataframe is sorted by the (ascending) odds of being in the positive class, so we can find the \"Trumpiest\" and \"least Trumpy\" tweets by saying ```.tail()``` or ```.head()``` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>fold</th>\n",
       "      <th>odds_positive</th>\n",
       "      <th>odds_negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>The invention of email has proven to be a very...</td>\n",
       "      <td>iphone</td>\n",
       "      <td>4</td>\n",
       "      <td>1.943128</td>\n",
       "      <td>-4.918958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134</th>\n",
       "      <td>Obama, and all others, have been so weak, and ...</td>\n",
       "      <td>android</td>\n",
       "      <td>4</td>\n",
       "      <td>1.962053</td>\n",
       "      <td>-5.507154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>The dishonest media didn't mention that Bernie...</td>\n",
       "      <td>android</td>\n",
       "      <td>4</td>\n",
       "      <td>1.962424</td>\n",
       "      <td>-5.452342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>Even though Bernie Sanders has lost his energy...</td>\n",
       "      <td>android</td>\n",
       "      <td>4</td>\n",
       "      <td>2.013873</td>\n",
       "      <td>-4.988697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1347</th>\n",
       "      <td>A big fat hit job on @oreillyfactor tonight. A...</td>\n",
       "      <td>android</td>\n",
       "      <td>4</td>\n",
       "      <td>2.014236</td>\n",
       "      <td>-5.993230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>Word is I am doing very well in Michigan and M...</td>\n",
       "      <td>android</td>\n",
       "      <td>4</td>\n",
       "      <td>2.050395</td>\n",
       "      <td>-6.183038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1369</th>\n",
       "      <td>The reason that Ted Cruz lost the Evangelicals...</td>\n",
       "      <td>android</td>\n",
       "      <td>4</td>\n",
       "      <td>2.086914</td>\n",
       "      <td>-5.638276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>I hate to say it, but the Republican Conventio...</td>\n",
       "      <td>android</td>\n",
       "      <td>4</td>\n",
       "      <td>2.102557</td>\n",
       "      <td>-5.473023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1127</th>\n",
       "      <td>Lyin' Ted Cruz denied that he had anything to ...</td>\n",
       "      <td>android</td>\n",
       "      <td>4</td>\n",
       "      <td>2.114487</td>\n",
       "      <td>-6.330089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>The Inspector General's report on Crooked Hill...</td>\n",
       "      <td>android</td>\n",
       "      <td>4</td>\n",
       "      <td>2.144761</td>\n",
       "      <td>-6.036284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>Ted Cruz is mathematically out of winning the ...</td>\n",
       "      <td>android</td>\n",
       "      <td>4</td>\n",
       "      <td>2.159337</td>\n",
       "      <td>-5.297136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>While I believe I will clinch before Cleveland...</td>\n",
       "      <td>android</td>\n",
       "      <td>4</td>\n",
       "      <td>2.202434</td>\n",
       "      <td>-6.469975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>George Will, one of the most overrated politic...</td>\n",
       "      <td>android</td>\n",
       "      <td>4</td>\n",
       "      <td>2.264743</td>\n",
       "      <td>-6.151903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>It was just announced-by sources-that no charg...</td>\n",
       "      <td>android</td>\n",
       "      <td>4</td>\n",
       "      <td>2.292544</td>\n",
       "      <td>-5.877032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>So with all of the Obama tough talk on Russia ...</td>\n",
       "      <td>android</td>\n",
       "      <td>4</td>\n",
       "      <td>2.330046</td>\n",
       "      <td>-6.548299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>Don King, and so many other African Americans ...</td>\n",
       "      <td>android</td>\n",
       "      <td>4</td>\n",
       "      <td>2.373351</td>\n",
       "      <td>-6.562629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1291</th>\n",
       "      <td>Marco Rubio lost big last night. I even beat h...</td>\n",
       "      <td>android</td>\n",
       "      <td>4</td>\n",
       "      <td>2.384545</td>\n",
       "      <td>-7.456906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>If Crooked Hillary Clinton can't close the dea...</td>\n",
       "      <td>android</td>\n",
       "      <td>4</td>\n",
       "      <td>2.398546</td>\n",
       "      <td>-6.573537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>The Crooked Hillary V.P. choice is VERY disres...</td>\n",
       "      <td>android</td>\n",
       "      <td>4</td>\n",
       "      <td>2.720438</td>\n",
       "      <td>-7.387543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>Crooked Hillary Clinton is unfit to serve as P...</td>\n",
       "      <td>android</td>\n",
       "      <td>4</td>\n",
       "      <td>2.826148</td>\n",
       "      <td>-8.252225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text   source  fold  \\\n",
       "162   The invention of email has proven to be a very...   iphone     4   \n",
       "1134  Obama, and all others, have been so weak, and ...  android     4   \n",
       "104   The dishonest media didn't mention that Bernie...  android     4   \n",
       "186   Even though Bernie Sanders has lost his energy...  android     4   \n",
       "1347  A big fat hit job on @oreillyfactor tonight. A...  android     4   \n",
       "1236  Word is I am doing very well in Michigan and M...  android     4   \n",
       "1369  The reason that Ted Cruz lost the Evangelicals...  android     4   \n",
       "154   I hate to say it, but the Republican Conventio...  android     4   \n",
       "1127  Lyin' Ted Cruz denied that he had anything to ...  android     4   \n",
       "679   The Inspector General's report on Crooked Hill...  android     4   \n",
       "962   Ted Cruz is mathematically out of winning the ...  android     4   \n",
       "1120  While I believe I will clinch before Cleveland...  android     4   \n",
       "438   George Will, one of the most overrated politic...  android     4   \n",
       "367   It was just announced-by sources-that no charg...  android     4   \n",
       "66    So with all of the Obama tough talk on Russia ...  android     4   \n",
       "536   Don King, and so many other African Americans ...  android     4   \n",
       "1291  Marco Rubio lost big last night. I even beat h...  android     4   \n",
       "798   If Crooked Hillary Clinton can't close the dea...  android     4   \n",
       "198   The Crooked Hillary V.P. choice is VERY disres...  android     4   \n",
       "346   Crooked Hillary Clinton is unfit to serve as P...  android     4   \n",
       "\n",
       "      odds_positive  odds_negative  \n",
       "162        1.943128      -4.918958  \n",
       "1134       1.962053      -5.507154  \n",
       "104        1.962424      -5.452342  \n",
       "186        2.013873      -4.988697  \n",
       "1347       2.014236      -5.993230  \n",
       "1236       2.050395      -6.183038  \n",
       "1369       2.086914      -5.638276  \n",
       "154        2.102557      -5.473023  \n",
       "1127       2.114487      -6.330089  \n",
       "679        2.144761      -6.036284  \n",
       "962        2.159337      -5.297136  \n",
       "1120       2.202434      -6.469975  \n",
       "438        2.264743      -6.151903  \n",
       "367        2.292544      -5.877032  \n",
       "66         2.330046      -6.548299  \n",
       "536        2.373351      -6.562629  \n",
       "1291       2.384545      -7.456906  \n",
       "798        2.398546      -6.573537  \n",
       "198        2.720438      -7.387543  \n",
       "346        2.826148      -8.252225  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newtestset.tail(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.\n",
    "\n",
    "To start with, just play around with the functions above in order to find a value of ```p``` (number of parameters in the model) that roughly maximizes accuracy on the test set.\n",
    "\n",
    "What accuracy do you get if you train a model on the whole ```tdf``` data frame, and also apply it to ```tdf``` as a whole?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.\n",
    "\n",
    "Write a function that *cross-validates* a modeling strategy by applying it successively to five different training sets and testing it on five different test sets.\n",
    "\n",
    "This is called \"five-fold crossvalidation.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 (probably, for homework).\n",
    "\n",
    "Do all this for the poefic dataset, trying to distinguish poetry from fiction. Create a new notebook. Copy functions as needed in order to build a naive Bayes classifier and run five-fold crossvalidation.\n",
    "\n",
    "How much accuracy do you get? Why do you think that accuracy is higher or lower than it was on the Trump tweet data? (You might want to inspect the data itself, using Excel or a text editor.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
