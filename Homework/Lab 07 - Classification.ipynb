{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 07\n",
    "\n",
    "This week, we'll learn a number of smaller skills:\n",
    "\n",
    "1. Getting Extracted Feature Files\n",
    "2. The Scipy Stack\n",
    "3. Pandas: Combining DataFrames\n",
    "4. Classification with Scikit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Extracted Features Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we learned in the datasets class, there are 14 million books in the Hathitrust Extracted Features Dataset.\n",
    "\n",
    "To download the EF dataset file for a book, you need its HathiTrust ID. You can see this in the URL when you find books in the HathiTrust; e.g. for the Tom Sawyer book at https://babel.hathitrust.org/cgi/pt?id=nyp.33433042068894, the id is nyp.33433042068894.\n",
    "\n",
    "With the ID you can download the file at the following URL:\n",
    "\n",
    "   > `https://bedrock.resnet.cms.waikato.ac.nz/vol-checker/VolumeCheck?download-id={{VOLUME ID}}`\n",
    "\n",
    "For example:\n",
    "\n",
    "   > https://bedrock.resnet.cms.waikato.ac.nz/vol-checker/VolumeCheck?download-id=nyp.33433042068894\n",
    "\n",
    "Two things that we aren't focusing on, but which you can explore if you want large numbers of files for your final projects:\n",
    "- There are many ways to programmatically choose _many_ books at once, rather than looking up the books in the online interface. The easiest is to download the bibliographic metadata (called [Hathifile](https://www.hathitrust.org/hathifiles). This is a CSV file of all the available books. \n",
    "- The main way to download files or lists of files is using a command line application called `rsync`, after converting the IDs to a file path. The reference for doing so is here: [Syncing a list of files](https://github.com/htrc/htrc-feature-reader/blob/master/examples/ID_to_Rsync_Link.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Scipy Stack\n",
    "\n",
    "You guys are becoming Pandas pros! Pandas is the foundation of much data science work by professionals today. As we continue, increasingly we'll be learning Pandas-specific code and conventions rather than all of Python.\n",
    "\n",
    "Pandas is part of what is called the SciPy Stack: a selection of scientific tools that all work together. Here are the other ones:\n",
    "\n",
    " - **Numpy**: A mathematical library, offering ways to represent multidimensional arrays.\n",
    " - **Scipy**: Foundational scientific code.\n",
    " - **Pandas**: A nicer way of structuring and working with data, through DataFrame and Series objects. You can think of Pandas as a more flexible version of Numpy's arrays, where you can name the columns and work with the data with more semantics.\n",
    " - **Matplotlib**: A visualization library. We've seen this!\n",
    " - **IPython**: This is the special interactive version of Python that you use in Jupyter! So you don't have to write a script, run it, edit it, run it again, and so on.\n",
    " \n",
    "There are many tools that run very well with the SciPy stack and round out our data science environment:\n",
    "\n",
    "- **Scikit Learn**: Where Scipy is foundation tools, Scikit Learn gives you many advanced scientific algorithms for data science. Great documentation too: next week's clustering reading is from their documentation.\n",
    "- **Jupyter**: The web browser notebook-style way of using IPython.\n",
    "- **Seaborn**: Higher-level visualization tools. Matplotlib is like buying IKEA furniture: some assembly required. Seaborn is like buying pre-assembled furniture: much easier! Plus, just importing Seaborn into your code makes your Matplotlib code look nicer!\n",
    "- **Statmodels**: Similar to Scipy, statsmodels offers additional statistics models and tools.\n",
    "\n",
    "There are a few benefits due to how standard these tools are. First, they tend to play nice together. If you move into advanced libraries for really large scale analysis, those libraries tend to have the SciPy stack in mind too. Finally, they were installed by default when you installed Anaconda in the first week, so you have them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas: Combining DataFrames\n",
    "\n",
    "To combine multiple DataFrames, you can use `pd.concat()` on a list of DataFrames (i.e. [dataframe1, dataframe2, etc.] ). For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1\n",
       "0  1  a\n",
       "1  2  b"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test = pd.DataFrame([(1,'a'), (2,'b')])\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1\n",
       "0  1  a\n",
       "1  2  b\n",
       "0  1  a\n",
       "1  2  b"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combining two of the same dataframe:\n",
    "list_of_dataframes = [test, test]\n",
    "combined = pd.concat(list_of_dataframes)\n",
    "combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classification with Scikit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've prepared a set of training documents and testing documents for a French/English classifier in [english_french_class.csv](https://raw.githubusercontent.com/organisciak/Text-Mining-Course/master/data/classification/english_french_class.csv).\n",
    "\n",
    "Load a CSV to a dataframe as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>!—</th>\n",
       "      <th>!—the</th>\n",
       "      <th>\"</th>\n",
       "      <th>\"\"</th>\n",
       "      <th>\"because</th>\n",
       "      <th>\"if</th>\n",
       "      <th>\"it</th>\n",
       "      <th>\"only</th>\n",
       "      <th>\"or</th>\n",
       "      <th>...</th>\n",
       "      <th>ﬂight</th>\n",
       "      <th>ﬂights</th>\n",
       "      <th>ﬂoor</th>\n",
       "      <th>ﬂown</th>\n",
       "      <th>ﬂuid</th>\n",
       "      <th>ﬂung</th>\n",
       "      <th>ﬂush</th>\n",
       "      <th>ﬂushed</th>\n",
       "      <th>ﬂy</th>\n",
       "      <th>ﬂying</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hvd.32044014292023</th>\n",
       "      <td>868.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4582.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hvd.32044102860673</th>\n",
       "      <td>1354.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 16115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         !   !—  !—the       \"   \"\"  \"because   \"if   \"it  \\\n",
       "book                                                                        \n",
       "hvd.32044014292023   868.0  0.0    0.0  4582.0  2.0       6.0  10.0  22.0   \n",
       "hvd.32044102860673  1354.0  0.0    0.0   139.0  0.0       0.0   0.0   0.0   \n",
       "\n",
       "                    \"only  \"or  ...    ﬂight  ﬂights  ﬂoor  ﬂown  ﬂuid  ﬂung  \\\n",
       "book                            ...                                            \n",
       "hvd.32044014292023    7.0  6.0  ...      0.0     0.0   0.0   0.0   0.0   0.0   \n",
       "hvd.32044102860673    0.0  0.0  ...      0.0     0.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "                    ﬂush  ﬂushed   ﬂy  ﬂying  \n",
       "book                                          \n",
       "hvd.32044014292023   0.0     0.0  0.0    0.0  \n",
       "hvd.32044102860673   0.0     0.0  0.0    0.0  \n",
       "\n",
       "[2 rows x 16115 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/organisciak/Text-Mining-Course/master/data/classification/english_french_class.csv'\n",
    "data = pd.read_csv(url, encoding='utf-8').set_index('book')\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loaded a 'wide' DataFrame, where each row is a book, each column is a word, and the cell $value_{row, column}$ is the count for that $word_{column}$ in $book_{row}$. The book ids were also a column, but we converted those to an index after loading with `set_index('book')`. I'll detail later how this information was collected.\n",
    "\n",
    "There is also a CSV with the [truth labels](https://raw.githubusercontent.com/organisciak/Text-Mining-Course/master/data/classification/english_french_class_labels.csv) for each book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>title</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hvd.32044014292023</td>\n",
       "      <td>Alice's adventures in Wonderland ; and, Throug...</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hvd.32044102860673</td>\n",
       "      <td>Notre Dame de Paris. Abridged and edited, with...</td>\n",
       "      <td>fre</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 book                                              title  \\\n",
       "0  hvd.32044014292023  Alice's adventures in Wonderland ; and, Throug...   \n",
       "1  hvd.32044102860673  Notre Dame de Paris. Abridged and edited, with...   \n",
       "\n",
       "  language  \n",
       "0      eng  \n",
       "1      fre  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/organisciak/Text-Mining-Course/master/data/classification/english_french_class_labels.csv'\n",
    "labels = pd.read_csv(url, encoding='utf-8')\n",
    "labels.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For train/test, we'll use half of the documents to build a classifier and the other half to test it.\n",
    "\n",
    "(`iloc` allows you to slice dataframes by number; e.g. `iloc[0:6]`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = data.iloc[0:6]\n",
    "train_labels = labels.iloc[0:6]\n",
    "# print(train_data)\n",
    "# print(train_labels)\n",
    "\n",
    "test_data = data.iloc[6:]\n",
    "test_labels = labels.iloc[6:]\n",
    "# print(test_data)\n",
    "# print(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes classification is straightforward to use with Scikit Learn. To train, you need data and the correct classes:\n",
    "\n",
    "```python \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(training_data, training_labels)\n",
    "```\n",
    "\n",
    "To predict the class for unknown books, format their word frequencies in the same order and pass the information to the classifier:\n",
    "\n",
    "```python\n",
    "classifier.predict(new_data)\n",
    "```\n",
    "\n",
    "Lets try it for real:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "# Train our model!\n",
    "classifier.fit(train_data, train_labels['language'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now load a new set of books and predict them. The books of the test dataset are:\n",
    "\n",
    "1. Les caves du Vatican\n",
    "2. Madame Bovary\n",
    "3. Jean Barois\n",
    "4. Catch-22\n",
    "5. The Catcher in the Rye\n",
    "6. The Lord of the Rings\n",
    "\n",
    "Let's predict their languages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['fre', 'fre', 'fre', 'eng', 'eng', 'eng'], \n",
       "      dtype='<U3')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the language of another book\n",
    "classifier.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect classification!\n",
    "\n",
    "For most classification tasks, the accuracy is lower. Languages are very distinct, however. You can see the underlying information from the classifier with `classifier.predict_log_proba(test_data)`: zero shows the chosen class and the closer to zero the other values are, the closer their class probability was to the one that was eventually selected.\n",
    "\n",
    "These books are 'test' documents because we know the real answer. The true labels can be given to `classifier.score`, to count what proportion of classifications are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(test_data.values, test_labels['language'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There isn't much complexity to this code. The tricky parts are in getting the data structured properly. Scikit Learn doesn't keep the column names from Pandas, it just pulls out the values. So if your training data looks like:\n",
    "\n",
    "```document 1: [word X count, word Y count, ... word Z]\n",
    "document 2: [word X count, word Y count, ... word Z]```\n",
    "\n",
    "Then you need to make sure that your future documents order their counts as X, Y, ... Z.\n",
    "\n",
    "To better see the information Scikit Learn is using, consider this test DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ii</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    A  B  C\n",
       "i   1  2  3\n",
       "ii  4  5  6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.DataFrame([[1,2,3], [4,5,6]], columns=['A', 'B', 'C'], index=['i', 'ii'])\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the information that the classifier actually uses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like a list of lists, doesn't it? It's a Numpy array, which for now you can think as a smarter, faster version of a list of lists. What matters is remembering that it is just numbers arranged in two dimensions, so don't expect the classifier to know what word each number refers to.\n",
    "\n",
    "The truth labels are just one dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['fre', 'fre', 'fre', 'eng', 'eng', 'eng'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels['language'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the following data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = 'https://raw.githubusercontent.com/organisciak/Text-Mining-Course/master/data/contemporary_books/'\n",
    "author_data = pd.read_csv(path + 'contemporary.csv', encoding='utf-8').set_index('book')\n",
    "author_labels = pd.read_csv(path + 'contemporary_labels.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll be building a classifier for author."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1**: What are the 3 classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mdp.39015005028686</td>\n",
       "      <td>King</td>\n",
       "      <td>The stand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mdp.39015010763418</td>\n",
       "      <td>Atwood</td>\n",
       "      <td>Lady oracle;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mdp.39015027242315</td>\n",
       "      <td>Atwood</td>\n",
       "      <td>The robber bride</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mdp.39015029244657</td>\n",
       "      <td>Grisham</td>\n",
       "      <td>The pelican brief</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mdp.39015031703609</td>\n",
       "      <td>Grisham</td>\n",
       "      <td>The rainmaker</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 book   author              title\n",
       "0  mdp.39015005028686     King          The stand\n",
       "1  mdp.39015010763418   Atwood       Lady oracle;\n",
       "2  mdp.39015027242315   Atwood   The robber bride\n",
       "3  mdp.39015029244657  Grisham  The pelican brief\n",
       "4  mdp.39015031703609  Grisham      The rainmaker"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_labels.head()\n",
    "\n",
    "#Atwood, King and Grisham?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2**: Show the code to split the data and truth labels into a test and train dataset, using the first fifteen books for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mdp.39015005028686</td>\n",
       "      <td>King</td>\n",
       "      <td>The stand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mdp.39015010763418</td>\n",
       "      <td>Atwood</td>\n",
       "      <td>Lady oracle;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mdp.39015027242315</td>\n",
       "      <td>Atwood</td>\n",
       "      <td>The robber bride</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mdp.39015029244657</td>\n",
       "      <td>Grisham</td>\n",
       "      <td>The pelican brief</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mdp.39015031703609</td>\n",
       "      <td>Grisham</td>\n",
       "      <td>The rainmaker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mdp.39015038148048</td>\n",
       "      <td>King</td>\n",
       "      <td>Desperation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mdp.39015040702071</td>\n",
       "      <td>Atwood</td>\n",
       "      <td>Alias Grace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mdp.39015043780249</td>\n",
       "      <td>King</td>\n",
       "      <td>The girl who loved Tom Gordon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mdp.39015043798936</td>\n",
       "      <td>King</td>\n",
       "      <td>Bag of bones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mdp.39015046381565</td>\n",
       "      <td>Grisham</td>\n",
       "      <td>A time to kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mdp.39015046788223</td>\n",
       "      <td>Grisham</td>\n",
       "      <td>The rainmaker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mdp.39015046835560</td>\n",
       "      <td>Grisham</td>\n",
       "      <td>The partner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mdp.39015054084192</td>\n",
       "      <td>Grisham</td>\n",
       "      <td>The testament</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mdp.39015054263903</td>\n",
       "      <td>King</td>\n",
       "      <td>Everything's eventual : 14 dark tales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mdp.39015055831070</td>\n",
       "      <td>King</td>\n",
       "      <td>From a Buick 8 : a novel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  book   author                                  title\n",
       "0   mdp.39015005028686     King                              The stand\n",
       "1   mdp.39015010763418   Atwood                           Lady oracle;\n",
       "2   mdp.39015027242315   Atwood                       The robber bride\n",
       "3   mdp.39015029244657  Grisham                      The pelican brief\n",
       "4   mdp.39015031703609  Grisham                          The rainmaker\n",
       "5   mdp.39015038148048     King                            Desperation\n",
       "6   mdp.39015040702071   Atwood                            Alias Grace\n",
       "7   mdp.39015043780249     King          The girl who loved Tom Gordon\n",
       "8   mdp.39015043798936     King                           Bag of bones\n",
       "9   mdp.39015046381565  Grisham                         A time to kill\n",
       "10  mdp.39015046788223  Grisham                          The rainmaker\n",
       "11  mdp.39015046835560  Grisham                            The partner\n",
       "12  mdp.39015054084192  Grisham                          The testament\n",
       "13  mdp.39015054263903     King  Everything's eventual : 14 dark tales\n",
       "14  mdp.39015055831070     King               From a Buick 8 : a novel"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_author_data = author_data.iloc[0:15]\n",
    "train_author_labels = author_labels.iloc[0:15]\n",
    "train_author_labels.head(20)\n",
    "\n",
    "# test_author_data = author_data.iloc[15:]\n",
    "# test_author_labels = author_labels.iloc[15:]\n",
    "# test_author_labels.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3**: Create and train a Multinomial Naive Bayes classifier. Paste your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_author_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-c2835579a0f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Run classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_author_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_author_data' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "# Train\n",
    "classifier.fit(train_author_data, train_author_labels['author'])\n",
    "\n",
    "# Run classifier\n",
    "classifier.predict(test_author_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4**: What are the classifier's predictions on the test code? Fill in the numbered values below:\n",
    "\n",
    "```\n",
    "array(['Grisham', '[[1]]', 'Atwood', 'King', 'Grisham', '[[2]]', 'King',\n",
    "       'Grisham', 'King', '[[3]]', 'Atwood', 'Atwood', 'Grisham', 'King',\n",
    "       '[[4]]', 'King'], \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers:\n",
    "\n",
    "1. Atwood\n",
    "2. Grisham\n",
    "3. Atwood\n",
    "4. Atwood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5**: What is the classifier's accuracy to four decimal places? i.e. X.XXXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier.score(test_author_data.values, test_author_labels['author'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6**: Which books were classified correctly or incorrectly:\n",
    "\n",
    "- Cell by Stephen King: [Correct, Incorrect]\n",
    "- The Handmaid's Tale by Margaret Atwood: [Correct, Incorrect]\n",
    "- Danse macabre by Stephen King: [Correct, Incorrect]\n",
    "- Cat's eye by Margaret Atwood: [Correct, Incorrect]\n",
    "\n",
    "_Make sure you're getting the correct answer before continuing._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Q6\n",
    "pred_author = classifier.predict(test_author_data)\n",
    "print(pred_author)\n",
    "\n",
    "test_author_labels.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7**: Build a classification report with SciKit Learn as [shown here](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#evaluation-of-the-performance-on-the-test-set). The 'names' of classes are in `classified.classes_`. Fill in the missing precision and recall values below:\n",
    "\n",
    "```\n",
    "             precision    recall    f1-score   support\n",
    "\n",
    "     Atwood       [[1]]     1.00       0.91         5\n",
    "    Grisham       1.00      [[2]]      1.00         5\n",
    "       King       1.00      0.83       0.91         6\n",
    "\n",
    "avg / total       0.95      [[3]]      0.94        16\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# ?metrics.classification_report\n",
    "\n",
    "names = classifier.classes_\n",
    "\n",
    "print(metrics.classification_report(test_author_labels['author'], pred_author, target_names=names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8**: Try to train a new classifier, without smoothing. In other words, set classifier.alpha to 0 before training. What happens to the predictions and what do the underlying probability patterns point to as the reason?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.alpha = 0\n",
    "\n",
    "# Train\n",
    "classifier.fit(train_author_data, train_author_labels['author'])\n",
    "\n",
    "# Run classifier\n",
    "pred_auth2 = classifier.predict(test_author_data)\n",
    "pred_auth_train = classifier.predict(train_author_data)\n",
    "\n",
    "print(metrics.classification_report(test_author_labels['author'], pred_auth2, target_names=names))\n",
    "print(metrics.classification_report(train_author_labels['author'], pred_auth_train, target_names=names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q9**: [2 marks] [This printing of Sense and Sensibility](https://catalog.hathitrust.org/Record/008663968) has two volumes. I'm interested in looking at word patterns for the combined set of volumes all at once. How would you download the Extracted Features files for both volumes, and what code would you use to read and join the token count DataFrames into one long DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from htrc_features import FeatureReader\n",
    "import os\n",
    "\n",
    "# First, use the above URL (https://bedrock.resnet.cms.waikato.ac.nz/vol-checker/VolumeCheck?download-id=<vol ID>) twice to download EF files,\n",
    "#  changing out volume ID with 'nyp.33433074943592' and 'nyp.33433074943600' to get both volumes of Sense & Sensibility.\n",
    "\n",
    "path1 = [os.path.join('/Users/rdubnic2/Documents/lis590txl/Data', 'nyp.33433074943592.json.bz2')]\n",
    "fr1 = FeatureReader(path1)\n",
    "\n",
    "vol1 = fr1.first()\n",
    "print(vol1)\n",
    "\n",
    "vol1_tokens = vol1.tokenlist()\n",
    "print(len(vol1_tokens))\n",
    "\n",
    "path2 = [os.path.join('/Users/rdubnic2/Documents/lis590txl/Data', 'nyp.33433074943600.json.bz2')]\n",
    "fr2 = FeatureReader(path2)\n",
    "\n",
    "vol2 = fr2.first()\n",
    "print(vol2)\n",
    "\n",
    "vol2_tokens = vol2.tokenlist()\n",
    "print(len(vol2_tokens))\n",
    "\n",
    "sense_sensibility_vols = [vol1_tokens, vol2_tokens]\n",
    "\n",
    "s_and_s = pd.concat(sense_sensibility_vols)\n",
    "s_and_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Notes\n",
    "\n",
    "I mentioned that even importing Seaborn makes your matplotlib graphics prettier. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "made_up_data = pd.Series([1,2,1,2,3,0,1])\n",
    "made_up_data.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn\n",
    "made_up_data.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
